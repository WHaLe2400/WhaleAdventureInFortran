# 最后攻势！

## 核心目标
通过fortran复现简单的CNN网络，同时在手写数字数据集MNIST上进行训练和测试，达到较高的准确率。

## 任务分解
首先需要在fortran中实现卷积神经网络的基本组件，包括卷积层、池化层、激活函数和全连接层。
需要具有的功能：
- 前向传播：实现数据通过网络的前向传播过程。
- 反向传播：实现误差的反向传播以更新权重。
- 损失函数：实现交叉熵损失函数用于分类任务。
- 优化算法：实现梯度下降或其他优化算法来更新网络权重。

模块命名于接口：
- Convolution：实现卷积操作。
- Pooling：实现池化操作。
- Activation：实现激活函数（这里只要ReLU、Softmax、sigmoid）。
- FullyConnected：实现全连接层。

## 网络结构
设计一个简单的CNN网络结构：
- 输入层：28x28x1（MNIST图像）
- 卷积层1：32个3x3卷积核，步长1，ReLU激活
- 池化层1：2x2最大池化，步长2
- 卷积层2：64个3x3卷积核，步长1，ReLU激活
- 池化层2：2x2最大池化，步长2
- 全连接层1：128个神经元，ReLU激活
- 输出层：10个神经元，Softmax激活

## 数据集准备
使用MNIST数据集，包含60000张训练图片和10000张测试图片，每张图片为28x28的灰度图像，表示手写数字0-9。
首先从tensorflow或其他来源下载MNIST数据集，并将其转换为适合fortran处理的格式（如二进制文件或文本文件）。这一步使用python实现好了，毕竟这个数据集就是在python的numpy环境下构建的。（目前已经完成）

## 卷积层：
实现卷积操作的fortran模块，支持多通道输入和输出。
需要有前向传播和反向传播功能。

- 好吧，我必须承认我想的有点简单了，Fortran异常严格的变量命名规则以及神奇的继承逻辑让几个原本能分别快乐运行的代码放在一起之后，因为变量名冲突反复崩溃，博客上找了不少解释，都说我的写法是符合规范的，但是怎么就跑不起来呢？？？？

## 池化层：
实现最大池化操作的fortran模块。这个简单些，只需要前向传播功能。

## 激活函数：
实现ReLU和Softmax激活函数的fortran模块。

## 全连接层：
实现全连接层的fortran模块。

## 优化算法：
实现梯度下降优化算法的fortran模块。